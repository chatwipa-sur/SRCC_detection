{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chatwipa-sur/SRCC_detection/blob/main/Teacher_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXaiY1xleypn"
      },
      "source": [
        "# Trian teacher model\n",
        "- Train CenterNet model with original ground truth label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q-776yegIAP"
      },
      "outputs": [],
      "source": [
        "# Standard library for file and directory operations\n",
        "import os\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Library for parsing XML files (used for annotations)\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Core PyTorch library for tensor operations\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "# Library for numerical computations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Library for data visualization\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5_vYrHMAVOb"
      },
      "outputs": [],
      "source": [
        "# Mount the drive with google colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CHObEB97_8"
      },
      "source": [
        "## 1. Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYogrd2QKamT"
      },
      "outputs": [],
      "source": [
        "# Make heatmaps using the utility functions from the centernet repo\n",
        "# Draw a Gaussian centered at a point on a heatmap\n",
        "def draw_msra_gaussian(heatmap, center, sigma=2):\n",
        "  # Total size (radius) of the Gaussian region\n",
        "  tmp_size = sigma * 6\n",
        "\n",
        "  # Round the x and y center to nearest int\n",
        "  mu_x = int(center[0] + 0.5)\n",
        "  mu_y = int(center[1] + 0.5)\n",
        "\n",
        "  # Get heatmap dimensions\n",
        "  w, h = heatmap.shape[0], heatmap.shape[1]\n",
        "\n",
        "  # Upper-left and bottom-right corners of the Gaussian patch\n",
        "  ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n",
        "  br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n",
        "\n",
        "  # If Gaussian patch is completely outside the heatmap, return\n",
        "  if ul[0] >= h or ul[1] >= w or br[0] < 0 or br[1] < 0:\n",
        "    return heatmap\n",
        "\n",
        "  # Create a 2D Gaussian\n",
        "  size = 2 * tmp_size + 1 # Size of the Gaussian patch\n",
        "  x = np.arange(0, size, 1, np.float32) # x coordinates\n",
        "  y = x[:, np.newaxis] # y coordinates\n",
        "  x0 = y0 = size // 2 # Center of the Gaussian\n",
        "  g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2)) # 2D Gaussian formula\n",
        "\n",
        "  # Compute valid bounds for placing the Gaussian patch on the heatmap\n",
        "  g_x = max(0, -ul[0]), min(br[0], h) - ul[0]\n",
        "  g_y = max(0, -ul[1]), min(br[1], w) - ul[1]\n",
        "  img_x = max(0, ul[0]), min(br[0], h)\n",
        "  img_y = max(0, ul[1]), min(br[1], w)\n",
        "\n",
        "  # Place the Gaussian on the heatmap using max to retain highest intensity\n",
        "  heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]] = np.maximum(\n",
        "    heatmap[img_y[0]:img_y[1], img_x[0]:img_x[1]],\n",
        "    g[g_y[0]:g_y[1], g_x[0]:g_x[1]])\n",
        "\n",
        "  return heatmap\n",
        "\n",
        "# Draw a dense regression value map around a center point\n",
        "def draw_dense_reg(regmap, heatmap, center, value, radius, is_offset=False):\n",
        "  # Size of the square region\n",
        "  diameter = 2 * radius + 1\n",
        "  # Create Gaussian mask\n",
        "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
        "\n",
        "  # Convert value to float32 and reshape to [dim, 1, 1]\n",
        "  value = np.array(value, dtype=np.float32).reshape(-1, 1, 1)\n",
        "  dim = value.shape[0]\n",
        "\n",
        "  # Create a repeated value map of shape [dim, H, W]\n",
        "  reg = np.ones((dim, diameter*2+1, diameter*2+1), dtype=np.float32) * value\n",
        "\n",
        "  # If it's an offset, adjust the values using distance from center\n",
        "  if is_offset and dim == 2:\n",
        "    delta = np.arange(diameter*2+1) - radius\n",
        "    reg[0] = reg[0] - delta.reshape(1, -1)\n",
        "    reg[1] = reg[1] - delta.reshape(-1, 1)\n",
        "\n",
        "  # Center coordinates\n",
        "  x, y = int(center[0]), int(center[1])\n",
        "\n",
        "  # Heatmap dimensions\n",
        "  height, width = heatmap.shape[0:2]\n",
        "\n",
        "  # Compute boundaries (clipping at image edges)\n",
        "  left, right = min(x, radius), min(width - x, radius + 1)\n",
        "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
        "\n",
        "  # Crop regions from heatmap, regmap, Gaussian, and value maps\n",
        "  masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
        "  masked_regmap = regmap[:, y - top:y + bottom, x - left:x + right]\n",
        "  masked_gaussian = gaussian[radius - top:radius + bottom,\n",
        "                             radius - left:radius + right]\n",
        "  masked_reg = reg[:, radius - top:radius + bottom,\n",
        "                      radius - left:radius + right]\n",
        "\n",
        "  # Only update if region is valid (non-zero size)\n",
        "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug\n",
        "\n",
        "    # Create a mask where Gaussian is stronger than existing heatmap\n",
        "    idx = (masked_gaussian >= masked_heatmap).reshape(1, masked_gaussian.shape[0],\n",
        "                                                      masked_gaussian.shape[1])\n",
        "\n",
        "    # Update regmap only where mask is True (using Gaussian gate)\n",
        "    masked_regmap = (1-idx) * masked_regmap + idx * masked_reg\n",
        "\n",
        "  # Write updated region back to original regmap\n",
        "  regmap[:, y - top:y + bottom, x - left:x + right] = masked_regmap\n",
        "\n",
        "  return regmap\n",
        "\n",
        "def gaussian2D(shape, sigma=1):\n",
        "    # Get center and coordinate grid\n",
        "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
        "    y, x = np.ogrid[-m:m+1,-n:n+1]\n",
        "\n",
        "    # Apply 2D Gaussian function\n",
        "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
        "\n",
        "    # Remove near-zero values for stability\n",
        "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
        "\n",
        "    return h\n",
        "\n",
        "def make_hm_regr(target, input_size=512, MODEL_SCALE=4):\n",
        "\n",
        "    IN_SCALE = 1\n",
        "    # make output heatmap for single class\n",
        "    hm = np.zeros([input_size//MODEL_SCALE, input_size//MODEL_SCALE], dtype=np.float32)\n",
        "    # make regr heatmap\n",
        "    regr = np.zeros([2, input_size//MODEL_SCALE, input_size//MODEL_SCALE], dtype=np.float32)\n",
        "\n",
        "    if target.empty:\n",
        "        return hm, regr\n",
        "\n",
        "    center = np.array([target[\"x\"] + target[\"w\"]//2, target[\"y\"] + target[\"h\"]//2,\n",
        "                       target[\"w\"], target[\"h\"]]).T\n",
        "\n",
        "    # make a center point\n",
        "    # try gaussian points.\n",
        "    for c in center:\n",
        "        hm = draw_msra_gaussian(hm, [int(c[0])//MODEL_SCALE//IN_SCALE,\n",
        "                                     int(c[1])//MODEL_SCALE//IN_SCALE],\n",
        "                                sigma=np.clip(c[2]*c[3]//2000, 2, 4))\n",
        "\n",
        "    # convert targets to its center.\n",
        "    regrs = center[:, 2:]/input_size/IN_SCALE\n",
        "\n",
        "    # plot regr values to mask\n",
        "    for r, c in zip(regrs, center):\n",
        "        for i in range(-2, 3):\n",
        "            for j in range(-2, 3):\n",
        "                try:\n",
        "                    regr[:, int(c[0])//MODEL_SCALE//IN_SCALE+i,\n",
        "                         int(c[1])//MODEL_SCALE//IN_SCALE+j] = r\n",
        "                except:\n",
        "                    pass\n",
        "    regr[0] = regr[0].T; regr[1] = regr[1].T;\n",
        "\n",
        "    return hm, regr\n",
        "\n",
        "def pred2box(hm, regr, thresh=0.99, MODEL_SCALE=4, input_size=512):\n",
        "    pred = hm > thresh\n",
        "    pred_center = np.where(pred)\n",
        "    pred_r = regr[:, pred].T  # Shape: [N, 2] for w, h\n",
        "    boxes = []\n",
        "    scores = hm[pred]\n",
        "\n",
        "    for i, b in enumerate(pred_r):\n",
        "        cx = pred_center[1][i] * MODEL_SCALE\n",
        "        cy = pred_center[0][i] * MODEL_SCALE\n",
        "\n",
        "        w = b[0] * input_size\n",
        "        h = b[1] * input_size\n",
        "\n",
        "        x1 = cx - w /2\n",
        "        y1 = cy - h /2\n",
        "\n",
        "        arr = np.clip([x1, y1, w, h], 0, input_size)\n",
        "        boxes.append(arr)\n",
        "\n",
        "    return np.asarray(boxes), scores\n",
        "\n",
        "\n",
        "# Assuming `image` is a torch.Tensor of shape (3, H, W)\n",
        "def unnormalize_tensor(image):\n",
        "  mean = [0.485, 0.456, 0.406]\n",
        "  std  = [0.229, 0.224, 0.225]\n",
        "  for t, m, s in zip(image, mean, std):\n",
        "    t.mul_(s).add_(m)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFs2DR4HRMAV"
      },
      "outputs": [],
      "source": [
        "class SignetRingCellPatchCenterNetDataset(Dataset):\n",
        "  def __init__(self, image_dir, annotation_dir, patch_size=512, transform=None):\n",
        "    # Initialize parameters\n",
        "    self.patch_size = patch_size\n",
        "    self.stride = patch_size//2  # 50% overlap for 512x512 patches\n",
        "    self.transform = transform\n",
        "    self.image_information = []\n",
        "\n",
        "    # Loop over positive and negative folders\n",
        "    for class_type in ['positive']:\n",
        "      full_image_dir = os.path.join(image_dir, class_type)\n",
        "      full_anno_dir = os.path.join(annotation_dir, class_type)\n",
        "\n",
        "      # Iterate over image files in the folder\n",
        "      for fname in os.listdir(full_image_dir):\n",
        "        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "          # Collect image path\n",
        "          image_path = os.path.join(full_image_dir, fname)\n",
        "\n",
        "          # Collect annotation path\n",
        "          if class_type == \"positive\":\n",
        "            anno_filename = fname.replace('.png', '.xml').replace('.jpg', '.xml').replace('.jpeg', '.xml')\n",
        "            anno_path = os.path.join(full_anno_dir, anno_filename)\n",
        "          else:\n",
        "            anno_path = None  # no annotation for negatives\n",
        "\n",
        "          # Get the size of image\n",
        "          image = cv2.imread(image_path)\n",
        "          height, width = image.shape[:2]\n",
        "\n",
        "          # Generate sliding window patches\n",
        "          for top in range(0, height - self.patch_size + 1, self.stride):\n",
        "            for left in range(0, width - self.patch_size + 1, self.stride):\n",
        "              self.image_information.append({\n",
        "                  'image_path': image_path,\n",
        "                  'annotation_path': anno_path,\n",
        "                  'crop_box': (left, top, left + self.patch_size, top + self.patch_size)\n",
        "              })\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_information) # Total number of patches\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get image for this index\n",
        "    image_info = self.image_information[idx]\n",
        "    image = cv2.imread(image_info['image_path'])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Crop patch using numpy slicing: [top:bottom, left:right]\n",
        "    left, top, right, bottom = image_info['crop_box']\n",
        "    image_patch = image[top:bottom, left:right]\n",
        "\n",
        "    # Load and process annotations\n",
        "    boxes_list = []\n",
        "    if image_info['annotation_path'] is not None:\n",
        "        tree = ET.parse(image_info['annotation_path'])\n",
        "        root = tree.getroot()\n",
        "\n",
        "        crop_xmin, crop_ymin, crop_xmax, crop_ymax = image_info['crop_box']\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            if name == 'ring_cell_cancer':\n",
        "                bbox = obj.find('bndbox')\n",
        "                xmin = int(bbox.find('xmin').text)\n",
        "                ymin = int(bbox.find('ymin').text)\n",
        "                xmax = int(bbox.find('xmax').text)\n",
        "                ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "                if (xmin >= crop_xmin and xmax <= crop_xmax) and (ymin >= crop_ymin and ymax <= crop_ymax):\n",
        "                    new_xmin = xmin - crop_xmin\n",
        "                    new_ymin = ymin - crop_ymin\n",
        "                    new_xmax = xmax - crop_xmin\n",
        "                    new_ymax = ymax - crop_ymin\n",
        "                    boxes_list.append((new_xmin, new_ymin,\n",
        "                                       new_xmax - new_xmin,\n",
        "                                       new_ymax - new_ymin))\n",
        "\n",
        "    # Generate heatmap and regression map as before\n",
        "    if len(boxes_list) == 0:\n",
        "        hm = np.zeros([self.patch_size // 4, self.patch_size // 4], dtype=np.float32)\n",
        "        regr = np.zeros([2, self.patch_size // 4, self.patch_size // 4], dtype=np.float32)\n",
        "    else:\n",
        "        target = pd.DataFrame(boxes_list, columns=['x', 'y', 'w', 'h'])\n",
        "        hm, regr  = make_hm_regr(target)\n",
        "\n",
        "    # Apply transforms if any\n",
        "    if self.transform:\n",
        "        image_patch = self.transform(image_patch)\n",
        "\n",
        "    return image_patch, hm, regr, boxes_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R73yGgdr-Q8k"
      },
      "source": [
        "#### Visualize dataset\n",
        "- Check `SignetRingCellPatchDataset` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYTjtHP1gacq"
      },
      "outputs": [],
      "source": [
        "# Transform function\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert NumPy to PIL\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Path of dataset\n",
        "image_dir = \"/content/drive/MyDrive/Dissertation_Chatwipa/dataset/train\"\n",
        "annotation_dir = \"/content/drive/MyDrive/Dissertation_Chatwipa/dataset/annotations/train\"\n",
        "\n",
        "# Load dataset and patching\n",
        "dataset = SignetRingCellPatchCenterNetDataset(image_dir, annotation_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OURNT5DahDfF"
      },
      "outputs": [],
      "source": [
        "# Get image and corresponding data from dataset\n",
        "image, hm, regr, gt_boxes = dataset[15]\n",
        "print(image.shape)  # (3, H, W) – check image size\n",
        "\n",
        "# Unnormalize image (if normalized during preprocessing)\n",
        "image = unnormalize_tensor(image.clone())\n",
        "\n",
        "# Get predicted boxes from heatmap and regression outputs\n",
        "pred_boxes, _ = pred2box(hm, regr)\n",
        "\n",
        "# Convert tensor image to numpy\n",
        "image = image.clone().permute(1, 2, 0).numpy()\n",
        "image = (image * 255).astype(np.uint8)\n",
        "\n",
        "# Copy image for drawing boxes\n",
        "image_copy = image.copy()\n",
        "\n",
        "# Draw predicted boxes in red\n",
        "for box in pred_boxes:\n",
        "    x, y, w, h = map(int, box)\n",
        "    cv2.rectangle(image_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "# Draw ground truth boxes in green\n",
        "for box in gt_boxes:\n",
        "    x1, y1, w, h = map(int, box)\n",
        "    cv2.rectangle(image_copy, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n",
        "\n",
        "# Create subplot for visualization\n",
        "fig, axs = plt.subplots(1, 2, figsize=(9, 3.5))\n",
        "\n",
        "# Display image with predicted and GT boxes\n",
        "axs[0].imshow(image_copy)\n",
        "axs[0].set_title(\"Red: Predicted Boxes | Green: GT Boxes\")\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Display raw heatmap\n",
        "axs[1].imshow(hm, cmap='hot')\n",
        "axs[1].set_title(\"Heatmap\")\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8hvTFtODjC"
      },
      "outputs": [],
      "source": [
        "# Get image and corresponding data from dataset\n",
        "image, hm, regr, gt_boxes = dataset[10]\n",
        "print(image.shape)  # (3, H, W) – check image size\n",
        "\n",
        "# Unnormalize image (if normalized during preprocessing)\n",
        "image = unnormalize_tensor(image.clone())\n",
        "\n",
        "# Get predicted boxes from heatmap and regression outputs\n",
        "pred_boxes, _ = pred2box(hm, regr)\n",
        "\n",
        "# Convert tensor image to numpy\n",
        "image = image.clone().permute(1, 2, 0).numpy()\n",
        "image = (image * 255).astype(np.uint8)\n",
        "\n",
        "# Copy image for drawing boxes\n",
        "image_copy = image.copy()\n",
        "\n",
        "# Draw predicted boxes in red\n",
        "for box in pred_boxes:\n",
        "    x, y, w, h = map(int, box)\n",
        "    cv2.rectangle(image_copy, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "# Draw ground truth boxes in green\n",
        "for box in gt_boxes:\n",
        "    x1, y1, w, h = map(int, box)\n",
        "    cv2.rectangle(image_copy, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n",
        "\n",
        "# Create subplot for visualization\n",
        "fig, axs = plt.subplots(1, 2, figsize=(9, 3.5))\n",
        "\n",
        "# Display image with predicted and GT boxes\n",
        "axs[0].imshow(image_copy)\n",
        "axs[0].set_title(\"Red: Predicted Boxes | Green: GT Boxes\")\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Display raw heatmap\n",
        "axs[1].imshow(hm, cmap='hot')\n",
        "axs[1].set_title(\"Heatmap\")\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAcPmsb6OZDS"
      },
      "source": [
        "## 2. Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpgXGZjZe3vp"
      },
      "outputs": [],
      "source": [
        "# Transform function\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Paths\n",
        "dataset_base_dir = \"/content/drive/MyDrive/Dissertation_Chatwipa/dataset\"\n",
        "\n",
        "# Train and Validation dataset\n",
        "train_dataset = SignetRingCellPatchCenterNetDataset(\n",
        "    image_dir=os.path.join(dataset_base_dir, \"train\"),\n",
        "    annotation_dir=os.path.join(dataset_base_dir, \"annotations/train\"),\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = SignetRingCellPatchCenterNetDataset(\n",
        "    image_dir=os.path.join(dataset_base_dir, \"val\"),\n",
        "    annotation_dir=os.path.join(dataset_base_dir, \"annotations/val\"),\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = SignetRingCellPatchCenterNetDataset(\n",
        "    image_dir=os.path.join(dataset_base_dir, \"test\"),\n",
        "    annotation_dir=os.path.join(dataset_base_dir, \"annotations/test\"),\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Define a custom collate function to handle the variable-length gt_boxes\n",
        "def custom_collate_fn(batch):\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    hms = torch.stack([torch.from_numpy(item[1]).float() for item in batch])\n",
        "    regrs = torch.stack([torch.from_numpy(item[2]).float() for item in batch])\n",
        "    gt_boxes = [item[3] for item in batch]\n",
        "\n",
        "    return images, hms, regrs, gt_boxes\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM-4iWVMAjJT"
      },
      "source": [
        "Check `train_loader` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3srGwJfiN7k"
      },
      "outputs": [],
      "source": [
        "for batch in train_loader:\n",
        "    if batch is None:\n",
        "        continue\n",
        "    images, hm, regr, gt_boxes = batch\n",
        "    print(f\"Loaded batch with {len(images)} images\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAFHcacbSAjL"
      },
      "outputs": [],
      "source": [
        "plt.imshow(hm[0], cmap='hot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXnO0vkwcp-R"
      },
      "source": [
        "Check `val_loader` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrGB-vdpcpU3"
      },
      "outputs": [],
      "source": [
        "for batch in val_loader:\n",
        "    if batch is None:\n",
        "        continue\n",
        "    images, hm, regr, gt_boxes = batch\n",
        "    print(f\"Loaded batch with {len(images)} images\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBiX_fvNRzkW"
      },
      "outputs": [],
      "source": [
        "plt.imshow(hm[0], cmap='hot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YPz60QBAuTf"
      },
      "source": [
        "Check `test_loader` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESzOoiBPArkX"
      },
      "outputs": [],
      "source": [
        "for batch in test_loader:\n",
        "    if batch is None:\n",
        "        continue\n",
        "    images, hm, regr, gt_boxes = batch\n",
        "    print(f\"Loaded batch with {len(images)} images\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7l7kjPkSDbR"
      },
      "outputs": [],
      "source": [
        "plt.imshow(hm[10], cmap='hot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0do4kXmAzuN"
      },
      "source": [
        "## 3. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMnUiTiKUb0N"
      },
      "source": [
        "### 3.1 CenterNet Model\n",
        "\n",
        "- The model is based on [CenterNet Baseline](https://www.kaggle.com/hocop1/centernet-baseline/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxPXNw0wcycK"
      },
      "outputs": [],
      "source": [
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2=None):\n",
        "        x1 = self.up(x1)\n",
        "        if x2 is not None:\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "            # input is CHW\n",
        "            diffY = x2.size()[2] - x1.size()[2]\n",
        "            diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "            x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                            diffY // 2, diffY - diffY//2))\n",
        "        else:\n",
        "            x = x1\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class centernet(nn.Module):\n",
        "    def __init__(self, n_classes=1, model_name=\"resnet18\", pretrain = True):\n",
        "        super(centernet, self).__init__()\n",
        "\n",
        "        # Create model backbone\n",
        "        if model_name == \"resnet50\":\n",
        "            basemodel = torchvision.models.resnet50(pretrained=pretrain)\n",
        "            num_ch = 2048\n",
        "        elif model_name == \"resnet34\":\n",
        "            basemodel = torchvision.models.resnet34(pretrained=pretrain)\n",
        "            num_ch = 512\n",
        "        elif model_name == \"resnet18\":\n",
        "            basemodel = torchvision.models.resnet18(pretrained=pretrain)\n",
        "            num_ch = 512\n",
        "\n",
        "        # Remove the last two layers (average pooling and linear layer)\n",
        "        self.base_model = nn.Sequential(*list(basemodel.children())[:-2])\n",
        "\n",
        "        # Freeze the backbone\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.up1 = up(num_ch, 512)\n",
        "        self.up2 = up(512, 256)\n",
        "        self.up3 = up(256, 256)\n",
        "\n",
        "        # Output classification\n",
        "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
        "\n",
        "        # Output residue\n",
        "        self.outr = nn.Conv2d(256, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        # Add positional info\n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.up3(x)\n",
        "        outc = self.outc(x)\n",
        "        outc = torch.sigmoid(outc)\n",
        "        outr = self.outr(x)\n",
        "        return outc, outr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCpxAOWDdzvL"
      },
      "source": [
        "### 3.2 Training Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwX8VaDdoad"
      },
      "source": [
        "#### Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc2bNsmAdr0V"
      },
      "outputs": [],
      "source": [
        "# From centernet repo\n",
        "# Modified focal loss to detect object centers via heatmap\n",
        "def neg_loss(pred, gt, pos_gamma=2, neg_gamma=4):\n",
        "  ''' Modified focal loss. Exactly the same as CornerNet.\n",
        "      Runs faster and costs a little bit more memory\n",
        "      Args:\n",
        "      pred: (B, 1, H, W)\n",
        "      gt: (B, 1, H, W)\n",
        "      pos_gamma: focusing parameter for positive examples\n",
        "      neg_gamma: focusing parameter for negative examples\n",
        "    '''\n",
        "\n",
        "  pred = pred.unsqueeze(1).float()\n",
        "  gt = gt.unsqueeze(1).float()\n",
        "\n",
        "  pos_inds = gt.eq(1).float() # Locations where there is a true object center\n",
        "  neg_inds = gt.lt(1).float()  # All other pixels\n",
        "  neg_weights = torch.pow(1 - gt, 4) # Higher weight for pixels farther from center\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  pos_loss = torch.log(pred + 1e-12) * torch.pow(1 - pred, pos_gamma) * pos_inds\n",
        "  neg_loss = torch.log(1 - pred + 1e-12) * torch.pow(pred, neg_gamma) * neg_weights * neg_inds\n",
        "\n",
        "  # pos_loss = torch.log(pred + 1e-12) * torch.pow(1 - pred, 3) * pos_inds\n",
        "  # neg_loss = torch.log(1 - pred + 1e-12) * torch.pow(pred, 3) * neg_weights * neg_inds\n",
        "\n",
        "  num_pos  = pos_inds.float().sum()\n",
        "  num_pos = torch.clamp(num_pos, min=1.0)\n",
        "\n",
        "  pos_loss = pos_loss.sum()\n",
        "  neg_loss = neg_loss.sum()\n",
        "\n",
        "  if num_pos == 0:\n",
        "    loss = loss - neg_loss\n",
        "  else:\n",
        "    loss = loss - (pos_loss + neg_loss) / num_pos\n",
        "\n",
        "  return loss\n",
        "\n",
        "# Smooth L1 regression loss for offsets, widths, etc.\n",
        "def _reg_loss(regr, gt_regr, mask):\n",
        "  ''' L1 regression loss\n",
        "    Arguments:\n",
        "      regr (batch x max_objects x dim)\n",
        "      gt_regr (batch x max_objects x dim)\n",
        "      mask (batch x max_objects)\n",
        "  '''\n",
        "  num = mask.float().sum()\n",
        "  #print(gt_regr.size())\n",
        "  mask = mask.sum(1).unsqueeze(1).expand_as(gt_regr)\n",
        "  #print(mask.size())\n",
        "\n",
        "  regr = regr * mask\n",
        "  gt_regr = gt_regr * mask\n",
        "\n",
        "  regr_loss = nn.functional.smooth_l1_loss(regr, gt_regr, size_average=False)\n",
        "  regr_loss = regr_loss / (num + 1e-4)\n",
        "\n",
        "  return regr_loss\n",
        "\n",
        "\n",
        "def centerloss(prediction, mask, regr, weight=0.5, size_average=True, eps=1e-6):\n",
        "    \"\"\"\n",
        "    Combines heatmap focal loss and regression L1 loss into a total loss.\n",
        "\n",
        "    Args:\n",
        "        prediction: (B, 3, H, W) where [:, 0] = heatmap, [:, 1:] = regr\n",
        "        mask: (B, H, W) – binary mask for object centers\n",
        "        regr: (B, 2, H, W) – regression ground truth\n",
        "        weight: float – weight of regression loss\n",
        "        size_average: bool – average the final loss across batch\n",
        "        eps: float – small number to avoid division by zero\n",
        "\n",
        "    Returns:\n",
        "        total_loss, mask_loss, regr_loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Heatmap and regression prediction\n",
        "    pred_mask = torch.sigmoid(prediction[:, 0])  # (B, H, W)\n",
        "    pred_regr = prediction[:, 1:]                # (B, 2, H, W)\n",
        "\n",
        "    # Heatmap focal loss\n",
        "    mask_loss = neg_loss(pred_mask, mask)\n",
        "\n",
        "    # Regression L1 loss, masked only where mask == 1\n",
        "    # Use broadcasting: (B, 2, H, W) * (B, 1, H, W)\n",
        "    mask_exp = mask.unsqueeze(1)  # (B, 1, H, W)\n",
        "    regr_loss = torch.abs(pred_regr - regr) * mask_exp\n",
        "    regr_loss = regr_loss.sum(dim=(1, 2, 3)) / (mask_exp.sum(dim=(1, 2, 3)) + eps)\n",
        "    regr_loss = regr_loss.mean()\n",
        "\n",
        "    # Combine\n",
        "    total_loss = mask_loss + regr_loss\n",
        "\n",
        "    if not size_average:\n",
        "        total_loss *= prediction.shape[0]\n",
        "\n",
        "    return total_loss, mask_loss, regr_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L1MP8cvW8yR"
      },
      "source": [
        "#### Define train and validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr9fbscjdym9"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize loss value\n",
        "    running_loss = 0.0\n",
        "    running_mask = 0.0\n",
        "    running_regr = 0.0\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():  # disable gradient calculations\n",
        "        for idx, (img, hm_gt, regr_gt, _) in enumerate(val_loader):\n",
        "            img = img.to(device)\n",
        "            hm_gt = hm_gt.to(device)\n",
        "            regr_gt = regr_gt.to(device)\n",
        "\n",
        "            hm_pred, regr_pred = model(img)\n",
        "\n",
        "            # Visualization: plot the GT and predicted heatmap for only first batch of epoch\n",
        "            if idx == 0 and (epoch % 2 == 0 or epoch == 0):\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.imshow(hm_gt[0].squeeze().cpu().detach().numpy(), cmap='hot')\n",
        "                plt.title('GT Heatmap')\n",
        "\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.imshow(torch.sigmoid(hm_pred[0]).squeeze().cpu().detach().numpy(), cmap='hot')\n",
        "                plt.title('Pred Heatmap')\n",
        "                plt.show()\n",
        "\n",
        "            preds = torch.cat((hm_pred, regr_pred), dim=1)\n",
        "            loss, mask_loss, regr_loss = centerloss(preds, hm_gt, regr_gt)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_mask += mask_loss.item()\n",
        "            running_regr += regr_loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_mask = running_mask / len(val_loader)\n",
        "    avg_regr = running_regr / len(val_loader)\n",
        "\n",
        "    print(f'Validation loss : {avg_loss:.4f}')\n",
        "    print(f'Validation mask loss : {avg_mask:.4f}')\n",
        "    print(f'Validation regr loss : {avg_regr:.4f}')\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    return avg_loss, avg_mask, avg_regr\n",
        "\n",
        "def train(epoch, save_dir):\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    print(f'epochs {epoch + 1}/{epochs}')\n",
        "\n",
        "    # Initialize loss value\n",
        "    running_loss = 0.0\n",
        "    running_mask = 0.0\n",
        "    running_regr = 0.0\n",
        "\n",
        "    # Use ASCII and static bar for GitHub/CI support\n",
        "    t = tqdm(train_loader, file=sys.stdout, dynamic_ncols=True)\n",
        "\n",
        "    # Iterate over batches\n",
        "    for idx, (img, hm_gt, regr_gt, _) in enumerate(t):\n",
        "\n",
        "        # Move batch tensors to the device (GPU or CPU)\n",
        "        img = img.to(device)\n",
        "        hm_gt = hm_gt.to(device)\n",
        "        regr_gt = regr_gt.to(device)\n",
        "\n",
        "        # Reset gradients for new batch\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "         # Forward pass: predict heatmap and regression outputs\n",
        "        hm_pred, regr_pred = model(img)\n",
        "\n",
        "        # Concatenate heatmap and regression predictions along channel dimension\n",
        "        preds = torch.cat((hm_pred, regr_pred), dim=1)\n",
        "\n",
        "        # Compute combined loss (total, mask, and regression)\n",
        "        loss, mask_loss, regr_loss = centerloss(preds, hm_gt, regr_gt)\n",
        "\n",
        "         # Visualization: plot the GT and predicted heatmap for only first batch of epoch\n",
        "        if idx == 0 and (epoch % 2 == 0 or epoch == 0):\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(hm_gt[0].squeeze().cpu().detach().numpy(), cmap='hot')\n",
        "            plt.title('GT Heatmap')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(torch.sigmoid(hm_pred[0]).squeeze().cpu().detach().numpy(), cmap='hot')\n",
        "            plt.title('Pred Heatmap')\n",
        "            plt.show()\n",
        "\n",
        "        # Backpropagation: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss values for reporting\n",
        "        running_loss += loss.item()\n",
        "        running_mask += mask_loss.item()\n",
        "        running_regr += regr_loss.item()\n",
        "\n",
        "         # Compute running average losses for progress bar display\n",
        "        avg_loss = running_loss / (idx + 1)\n",
        "        avg_mask = running_mask / (idx + 1)\n",
        "        avg_regr = running_regr / (idx + 1)\n",
        "\n",
        "        # Update tqdm progress bar with current average losses\n",
        "        t.set_postfix(loss=avg_loss, mask=avg_mask, regr=avg_regr)\n",
        "\n",
        "    # Average losses over the entire epoch after training completes\n",
        "    print(f'Train loss : {running_loss/len(train_loader):.4f}')\n",
        "    print(f'Train mask loss : {running_mask/len(train_loader):.4f}')\n",
        "    print(f'Train regr loss : {running_regr/len(train_loader):.4f}')\n",
        "\n",
        "    # Run validation after training epoch\n",
        "    val_loss, val_mask, val_regr = validate()\n",
        "\n",
        "    # Save full checkpoint every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': val_loss\n",
        "        }, f'{save_dir}/checkpoint_epoch_{epoch + 1}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLK5DQEKHt2n"
      },
      "source": [
        "### 3.3 Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HTZb_sjRW4O"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = ''\n",
        "TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwOyVUtvVo54"
      },
      "source": [
        "#### RMSprop and RestNet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "53PwqgMWHqkC"
      },
      "outputs": [],
      "source": [
        "# Check if it runs correctly\n",
        "model = centernet(model_name='resnet34')\n",
        "model(torch.rand(1,3,512,512))[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VreB_XjvV4K-"
      },
      "outputs": [],
      "source": [
        "# Gets the GPU if there is one, otherwise the cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhF-aQSuMPl5"
      },
      "outputs": [],
      "source": [
        "# Get current timestamp (format: YYYYMMDD_HHMMSS)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Create save directory with timestamp\n",
        "save_dir = f'/content/drive/MyDrive/Dissertation_Chatwipa/model_checkpoint/rmsprop34_checkpoints_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "epochs = 1\n",
        "if TRAIN:\n",
        "    for epoch in range(epochs):\n",
        "        train(epoch, save_dir)\n",
        "else:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd6W0iKvVkei"
      },
      "source": [
        "#### AdamW and RestNet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RjX16GIsUEc"
      },
      "outputs": [],
      "source": [
        "# Check if it runs correctly\n",
        "model = centernet(model_name='resnet34')\n",
        "model(torch.rand(1,3,512,512))[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W-GWfJFVc6-"
      },
      "outputs": [],
      "source": [
        "# Move model to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sf9xM-U3D6Mh"
      },
      "outputs": [],
      "source": [
        "# Get current timestamp (format: YYYYMMDD_HHMMSS)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Create save directory with timestamp\n",
        "save_dir = f'/content/drive/MyDrive/Dissertation_Chatwipa/model_checkpoint/adamw34_checkpoints_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "epochs = 5\n",
        "if TRAIN:\n",
        "    for epoch in range(epochs):\n",
        "        train(epoch, save_dir)\n",
        "else:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcgpm4gj2DFf"
      },
      "source": [
        "#### Adam and RestNet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evdaxqpn2DFg"
      },
      "outputs": [],
      "source": [
        "# Check if it runs correctly\n",
        "model = centernet(model_name='resnet34')\n",
        "model(torch.rand(1,3,512,512))[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFApGGV_2DFg"
      },
      "outputs": [],
      "source": [
        "# Move model to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvqk21WV2DFg"
      },
      "outputs": [],
      "source": [
        "# Get current timestamp (format: YYYYMMDD_HHMMSS)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Create save directory with timestamp\n",
        "save_dir = f'/content/drive/MyDrive/Dissertation_Chatwipa/model_checkpoint/adam34_checkpoints_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "epochs = 5\n",
        "if TRAIN:\n",
        "    for epoch in range(epochs):\n",
        "        train(epoch, save_dir)\n",
        "else:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO889DKXHfTT"
      },
      "source": [
        "### 3.4 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2xSDVDJHem-"
      },
      "outputs": [],
      "source": [
        "model = centernet(model_name='resnet34')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Dissertation_Chatwipa/model_checkpoint/adam34_checkpoints_20250606_093405/checkpoint_epoch_50.pth\"\n",
        "checkpoint = torch.load(model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82M6vdx8VZJR"
      },
      "source": [
        "Show prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOATsV-lVb4h"
      },
      "outputs": [],
      "source": [
        "for id in range(5):\n",
        "    # Get image and ground truth\n",
        "    image, hm_gt, regr_gt, gt_boxes = test_dataset[id]\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        hm_pred, regr_pred = model(image.to(device).float().unsqueeze(0))\n",
        "    hm_pred = hm_pred.cpu().numpy().squeeze(0).squeeze(0)\n",
        "    regr_pred = regr_pred.cpu().numpy().squeeze(0)\n",
        "\n",
        "    # Get predicted boxes\n",
        "    pred_boxes, _ = pred2box(hm_pred, regr_pred)\n",
        "\n",
        "    # Convert image to numpy\n",
        "    image_np = image.clone().permute(1, 2, 0).numpy()\n",
        "    image_np = (image_np * 255).astype(np.uint8)\n",
        "    image_with_boxes = image_np.copy()\n",
        "\n",
        "    # Draw predicted boxes (red)\n",
        "    for box in pred_boxes:\n",
        "        x, y, w, h = map(int, box)\n",
        "        cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    # Draw GT boxes (green)\n",
        "    for box in gt_boxes:\n",
        "        x1, y1, w, h = map(int, box)\n",
        "        cv2.rectangle(image_with_boxes, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n",
        "\n",
        "    # --- Create subplot ---\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 6))\n",
        "\n",
        "    # Image with boxes\n",
        "    axs[0].imshow(image_with_boxes)\n",
        "    axs[0].set_title(\"Red: Predicted | Green: Ground Truth\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    # Heatmap\n",
        "    axs[1].imshow(hm_pred, cmap='hot')\n",
        "    axs[1].set_title(\"Predicted Heatmap\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz97HiUumnTh"
      },
      "source": [
        "#### Post-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIPYf-zmmm1-"
      },
      "outputs": [],
      "source": [
        "# Pool duplicates by applying non-maximum suppression on 3x3 windows\n",
        "def pool(data):\n",
        "    stride = 3  # Step size for sliding window\n",
        "\n",
        "    # Loop over the heatmap with a stride of 3, avoiding borders\n",
        "    for y in np.arange(1, data.shape[1] - 1, stride):\n",
        "        for x in np.arange(1, data.shape[0] - 1, stride):\n",
        "\n",
        "            # Extract a 3x3 neighborhood around the current center (x, y)\n",
        "            a_2d = data[x - 1:x + 2, y - 1:y + 2]\n",
        "\n",
        "            # Find the index of the maximum value in the 3x3 window\n",
        "            max_value = np.asarray(np.unravel_index(np.argmax(a_2d), a_2d.shape))\n",
        "\n",
        "            # Loop over all positions in the 3x3 window\n",
        "            for c1 in range(3):\n",
        "                for c2 in range(3):\n",
        "                    # If the current position is not the maximum, suppress it\n",
        "                    if not (c1 == max_value[0] and c2 == max_value[1]):\n",
        "                        data[x + c1 - 1, y + c2 - 1] = -1  # Suppress by setting to -1\n",
        "\n",
        "    return data  # Return the modified heatmap with non-max values suppressed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNQ9Vg-xmbnP"
      },
      "outputs": [],
      "source": [
        "thresh = 0.6\n",
        "results = []\n",
        "\n",
        "for img, hm_gt, regr_gt, box_gt in tqdm(test_loader):\n",
        "\n",
        "    images = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        hms_pred, regrs_pred = model(images)\n",
        "\n",
        "    for img, hm, regr in zip(images, hms_pred, regrs_pred):\n",
        "        # process predictions\n",
        "        hm = hm.cpu().numpy().squeeze(0)\n",
        "        regr = regr.cpu().numpy()\n",
        "        hm = torch.sigmoid(torch.from_numpy(hm)).numpy()\n",
        "        hm = pool(hm)\n",
        "\n",
        "        boxes, scores = pred2box(hm, regr, thresh)\n",
        "\n",
        "        preds_sorted_idx = np.argsort(scores)[::-1]\n",
        "        boxes_sorted = boxes[preds_sorted_idx]\n",
        "        scores_sorted = scores[preds_sorted_idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Dummy function to undo normalization (if needed)\n",
        "def unnormalize_tensor(tensor):\n",
        "    # Assuming ImageNet normalization, adjust if needed\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1).to(tensor.device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1).to(tensor.device)\n",
        "    return tensor * std + mean\n",
        "\n",
        "# Start plotting\n",
        "for img, hm, regr, box_gt in tqdm(test_loader):\n",
        "\n",
        "    images = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hms_pred, regrs_pred = model(images)\n",
        "\n",
        "    for i, (img, hm, regr, gt_boxes) in enumerate(zip(images, hms_pred, regrs_pred, box_gt)):\n",
        "\n",
        "        # Heatmap and regression\n",
        "        hm = hm.cpu().numpy().squeeze(0)\n",
        "        regr = regr.cpu().numpy()\n",
        "        hm = torch.sigmoid(torch.from_numpy(hm)).numpy()\n",
        "        hm = pool(hm)\n",
        "\n",
        "        # Predict boxes\n",
        "        boxes, scores = pred2box(hm, regr, thresh)\n",
        "        print(boxes, scores)\n",
        "\n",
        "        # Sort predictions by confidence\n",
        "        preds_sorted_idx = np.argsort(scores)[::-1]\n",
        "        boxes_sorted = boxes[preds_sorted_idx]\n",
        "        scores_sorted = scores[preds_sorted_idx]\n",
        "\n",
        "        # Unnormalize image and convert for display\n",
        "        img_disp = img.cpu().clone().permute(1, 2, 0).numpy()\n",
        "        img_disp = (img_disp * 255).astype(np.uint8)\n",
        "        img_disp = img_disp.copy()\n",
        "\n",
        "        # Draw predicted boxes (red)\n",
        "        for box in boxes_sorted:\n",
        "            x, y, w, h = map(int, box)\n",
        "            cv2.rectangle(img_disp, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "        # Draw ground truth boxes (green)\n",
        "        for box in gt_boxes:\n",
        "            x, y, w, h = map(int, box)\n",
        "            cv2.rectangle(img_disp, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        # Show subplot with image and heatmap\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(img_disp)\n",
        "        plt.title(\"Red: Predicted | Green: Ground Truth\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(hm, cmap='hot')\n",
        "        plt.title(\"Predicted Heatmap\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Optionally, break early if just visualizing a few\n",
        "        if i > 1:\n",
        "            break\n",
        "    break  # Break outer loop to avoid showing all batches\n"
      ],
      "metadata": {
        "id": "Zf4FjNRihdqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN+8x8VJsGkKN5IBLIXhW2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}